
library(readr)
ub <- read_csv("C:/Users/YASH/Downloads/UniversalBank.csv")
head(ub)
str(ub)
ub.subset <- ub[c("Age",'Experience','Income','CCAvg','Education','Mortgage','Personal Loan','Securities Account','CD Account','Online','CreditCard')]

#Normalizing the data
normalize <- function (x) {return ((x - min(x))/(max(x) - min(x))) }
ub.subset.n <- as.data.frame ( lapply ( ub.subset [,-7] , normalize ) )
head ( ub.subset.n )

# random sample
set.seed ( 123 ) 

# Selecting 60 % data through random sampling.
data.1 <- sample(1:nrow(ub.subset.n),size = nrow(ub.subset.n) * 0.6, 
                 replace = FALSE )
# 60 % training data
train.ub <- ub.subset[ data.1, ]
# 30 % test data
test.ub <- ub.subset[ -data.1, ]


#Now creating seperate dataframe for ' Personal loan ' feature which is our target.

train.ub1 <- ub.subset[, 8]
test.ub1 <- ub.subset[-data.1 , 8 ]

install.packages("class")
# to install class packages as it carries kNN function
library (class) 
# to call class package

NROW ( train.ub1 )
# to find the number of observation

sqrt(3000)
# ~55

knn.55 <- knn(train = train.ub, test = test.ub, cl = train.ub1, k = 55 )
knn.54 <- knn(train = train.ub, test = test.ub, cl = train.ub1, k = 54 )


## Let's calculate the proportion of correct classification for k = 54, 55
ACC.55 <- 100 * sum ( test.ub1 == knn.55 ) / NROW ( test.ub1 )
ACC.54 <- 100 * sum ( test.ub1 == knn.54 ) / NROW ( test.ub1 )

ACC.54 #89.7
ACC.55 #89.9
# to check prediction 
table (knn.54, test.ub1)
table (knn.55, test.ub1)

install.packages ("caret")
library (caret)

#confusionMatrix

confusionMatrix (table(knn.55, test.ub1))
# declaration to initiate for loop
i= 1  
k.optm = 1
for ( i in 1:56 ) {
  knn.mod <- knn( train=train.ub, test=test.ub, cl=train.ub1, k = i )
  k.optm [ i ] <- 100 * sum (test.ub1 == knn.mod)/NROW(test.ub1)
  k = i
  cat (k,' = ',k.optm [ i ],' \ n ') # to print % accuracy
}
# plot % accuracy  to k - value
plot( k.optm, type = "point", xlab="K- Value",ylab="Accuracy level") 

#when k = 5 accuracy is the highest ~91

set.seed ( 123 ) # random sample
# Selecting 80 % data through random sampling.
data.2 <- sample(1:nrow(ub.subset.n),size = nrow(ub.subset.n) * 0.8, 
                 replace = FALSE )
# 80 % training data                 
train.ub.1 <- ub.subset[ data.2, ]
# 20 % test data
test.ub.1 <- ub.subset[ -data.2, ]

#Now creating seperate dataframe for ' Personal loan ' feature which is our target.

train.ub1.1 <- ub.subset[ data.2, 8]
test.ub1.1 <- ub.subset[ -data.2, 8 ]



NROW ( train.ub1.1 ) # to find the number of observation

sqrt(4000)
# ~63

knn.63 <- knn(train = train.ub.1, test = test.ub.1, cl = train.ub1.1, k = 63 )
knn.64 <- knn(train = train.ub.1, test = test.ub.1, cl = train.ub1.1, k = 64 )


## Let's calculate the proportion of correct classification for k = 54, 55
ACC.63 <- 100 * sum ( test.ub1.1 == knn.63 ) / NROW ( test.ub1.1 )
ACC.64 <- 100 * sum ( test.ub1.1 == knn.64 ) / NROW ( test.ub1.1 )

ACC.63 
#90.3
ACC.64 
#90.3
# prediction
table (knn.63, test.ub1.1) 
table (knn.64, test.ub1.1)

#confusionMatrix

confusionMatrix (table(knn.63, test.ub1.1))

i= 1   
k.optm = 1
for ( i in 1:100 ) {
  knn.mod <- knn( train=train.ub.1, test=test.ub.1, cl=train.ub1.1, k = i )
  k.optm [ i ] <- 100 * sum (test.ub1.1 == knn.mod)/NROW(test.ub1.1)
  k = i
  cat (k,' = ',k.optm [ i ],' \ n ') 
}
plot( k.optm, type = "point", xlab="K- Value",ylab="Accuracy level")
#plot % accuracy  to k - value





